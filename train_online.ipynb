{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place %autoreload 2 at the top of your notebook to ensure it applies to all subsequent imports.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import torch\n",
    "from trainer_end_to_end import Trainer\n",
    "from options import MonodepthOptions\n",
    "import time\n",
    "options = MonodepthOptions()\n",
    "args = [\n",
    "    '--num_epochs', '50000',\n",
    "    '--batch_size', '1',\n",
    "    '--log_frequency', '10',\n",
    "    '--save_frequency', '100000',\n",
    "    '--of_samples',\n",
    "    '--of_samples_num', '300', \n",
    "    # '--of_samples_num', '5', \n",
    "    # '--batch_size', '10', \n",
    "    '--enable_motion_computation', \n",
    "    '--use_raft_flow',\n",
    "    '--pose_model_type', 'endofast3r', \n",
    "    '--pose_model_type', 'posetr_net', \n",
    "    '--frame_ids', '0', '-4', '4',\n",
    "    '--frame_ids', '0', '-1', '1',\n",
    "    '--dataset', 'StereoMIS',\n",
    "    '--split_appendix', '_offline',\n",
    "    '--data_path', '/mnt/nct-zfs/TCO-All/SharedDatasets/StereoMIS_DARES_test/',\n",
    "    '--dataset', 'DynaSCARED',\n",
    "    '--split_appendix', '_CaToTi001',\n",
    "    # '--split_appendix', '_CaToTi101',\n",
    "    # '--split_appendix', '_CaToTi010',\n",
    "    '--data_path', '/mnt/cluster/datasets/Surg_oclr_stereo/',\n",
    "    '--dataset', 'StereoMIS',\n",
    "    '--split_appendix', '_offline',\n",
    "    '--data_path', '/mnt/nct-zfs/TCO-All/SharedDatasets/StereoMIS_DARES_test/',\n",
    "    '--motion_mask_thre_px', '1',\n",
    "    # '--frame_ids', '0', '-1', '1',\n",
    "    # '--pose_model_type', 'separate_resnet', \n",
    "    # '--log_frequency', '2',\n",
    "    # '--log_dir', '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/debug_reloc3r_backbone/relocxr',\n",
    "    # '--debug',\n",
    "    ]\n",
    "opts = options.parse_notebook(args)# trainer.set_eval()\n",
    "# # opts\n",
    "opts.datasets, opts.frame_ids, opts.pose_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb\n",
    "# self = trainer\n",
    "# self.train_dataset.frame_ids, self.opt.frame_ids, opts.frame_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "trainer = Trainer(opts)\n",
    "print('trainer initialized...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model for test\n",
    "import os\n",
    "traiend_models_root = '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/unfzD_SCARED_b3accu1_posetr_net_reloc3r_9D_OPTTranse01_ROTAng01_Loss_regTrMagWt0_regRotAngMagWt0_386to386rather4096_depthScratchDAM_motionDftFlow_allRegLoss_doubleB_loss2weight100_tanhandclampSample_deltaMax001/0918-212235/models/weights_4'\n",
    "traiend_models_root = '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/new_batch/b3accu4_scaredExcelSM_MonFast3r_trainedNfzD/1010-145110/models/weights_4'\n",
    "traiend_models_root = '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/new_batch/b3accu4_scaredcorrectedExcelSM_posetr_fzenc_trainedNfzD/1013-104014/models/weights_3'\n",
    "trained_models_root = '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/new_batch/b3accu4_scaredcorrectedExcelSM_posetr_fzenc_trainedNfzD_dft_DAonly/1014-185504/models/weights_4'\n",
    "trained_models_root = '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/new_batch/b3accu4_scaredcorrectedExcelSM_posetr_fzenc_trainedNfzD_learnedMskTrePx1_learnedlossR2Weight1_reproj2only/1014-173620/models/weights_4'\n",
    "\n",
    "\n",
    "print('pose_model_type:', opts.pose_model_type)\n",
    "if opts.pose_model_type == \"separate_resnet\":\n",
    "    pose_encoder_path = os.path.join(traiend_models_root, \"pose_encoder.pth\")\n",
    "    pose_decoder_path = os.path.join(traiend_models_root, \"pose.pth\")\n",
    "    assert os.path.exists(pose_encoder_path), f\"pose_encoder_path {pose_encoder_path} does not exist\"\n",
    "    assert os.path.exists(pose_decoder_path), f\"pose_decoder_path {pose_decoder_path} does not exist\"\n",
    "    trainer.models[\"pose_encoder\"].load_state_dict(torch.load(pose_encoder_path))\n",
    "    print(f\"loaded pose_encoder from {pose_encoder_path}\")\n",
    "    trainer.models[\"pose\"].load_state_dict(torch.load(pose_decoder_path))\n",
    "    print(f\"loaded pose_decoder from {pose_decoder_path}\")\n",
    "    # opts.enable_motion_computation = True\n",
    "\n",
    "\n",
    "    # load pretrained depth model\n",
    "    depth_model_path = os.path.join(traiend_models_root, \"depth_model.pth\")\n",
    "    depth_model_dict = torch.load(depth_model_path)\n",
    "    trainer.models[\"depth_model\"].load_state_dict(depth_model_dict)\n",
    "    print(f\"loaded depth_model from {depth_model_path}\")\n",
    "\n",
    "    # # load pretrained motion flow model\n",
    "    # motion_flow_model_path = os.path.join(traiend_models_root, \"motion_raft_flow.pth\")\n",
    "    # motion_flow_model_dict = torch.load(motion_flow_model_path)\n",
    "    # print(trainer.models.keys())\n",
    "    # # trainer.models[\"motion_raft_flow\"].load_state_dict(motion_flow_model_dict)\n",
    "    # # trainer.models[\"raft_flow\"].load_state_dict(motion_flow_model_dict)\n",
    "    # print(f\"loaded motion_flow_model from {motion_flow_model_path}\")\n",
    "elif opts.pose_model_type == \"endofast3r\" or opts.pose_model_type == \"posetr_net\":\n",
    "    # load trained models for evaluation\n",
    "    # refer to how resumed training is implemented in train_end_to_end.py\n",
    "    \n",
    "    # Load all models from the trained_models_root directory\n",
    "    for model_name, model in trainer.models.items():\n",
    "        model_path = os.path.join(traiend_models_root, f\"{model_name}.pth\")\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Loading {model_name} weights...\")\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_path)\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "        else:\n",
    "            print(f\"Warning: {model_name}.pth not found, skipping...\")\n",
    "    \n",
    "    print(\"EndoFASt3r model loading completed!\")\n",
    "\n",
    "else:\n",
    "    assert False, \"pose_model_type should be separate_resnet or posetr_net\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opts.pose_model_type\n",
    "trainer.opt.pose_model_type,trainer.opt.unireloc3r_pose_estimation_mode,trainer.opt.enable_all_depth,trainer.opt.enable_motion_computation,trainer.opt.enable_mutual_motion\n",
    "\n",
    "# trainer = Trainer(opts)\n",
    "self = trainer #Trainer(opts)\n",
    "\n",
    "\n",
    "# trainer.train()\n",
    "\"\"\"Run the entire training pipeline\n",
    "\"\"\"\n",
    "self.epoch = 0\n",
    "self.step = 0\n",
    "self.start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force no shuffle when test\n",
    "from torch.utils.data import DataLoader\n",
    "self.train_dataset.is_train = False # avoid flip\n",
    "\n",
    "\n",
    "# avoid shuffle\n",
    "self.train_loader = DataLoader(\n",
    "    self.train_dataset, self.opt.batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trainer = Trainer(opts)\n",
    "# self = trainer #Trainer(opts)\n",
    "\n",
    "\n",
    "# # trainer.train()\n",
    "# \"\"\"Run the entire training pipeline\n",
    "# \"\"\"\n",
    "# self.epoch = 0\n",
    "# # self.step = 0\n",
    "# self.start_time = time.time()\n",
    "\n",
    "#quick and convenient but confusing recontrooling!\n",
    "# self.opt.min_depth = 10.0 #risky\n",
    "# self.opt.min_depth = 0.1 #risky\n",
    "# self.opt.max_depth = 150.0\n",
    "# self.opt.gt_metric_rel_pose_as_estimates_debug = False\n",
    "\n",
    "# trained fast3r\n",
    "# depth: 0.2-0.3\n",
    "# rel_pose_per_axis: 2e-4\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for self.epoch in range(self.opt.num_epochs):\n",
    "    # self.run_epoch(self.epoch)\n",
    "    \"\"\"Run a single epoch of training and validation\n",
    "    \"\"\"\n",
    "    epoch = self.epoch\n",
    "    print(\"Training\")\n",
    "\n",
    "    self.model_optimizer_0.zero_grad()# debug only: save_mem\n",
    "    self.model_optimizer.zero_grad()# debug only: save_mem\n",
    "\n",
    "    for batch_idx, inputs in enumerate(self.train_loader):\n",
    "    # while True:\n",
    "\n",
    "        # train_iter = iter(self.train_loader)\n",
    "        # inputs = next(train_iter)\n",
    "\n",
    "\n",
    "\n",
    "        before_op_time = time.time()\n",
    "\n",
    "        # position\n",
    "        self.set_train_0()\n",
    "        if self.opt.debug:\n",
    "            self.freeze_params(keys = ['position_encoder'])#debug only to save_mem\n",
    "        _, losses_0 = self.process_batch_0(inputs)\n",
    "        self.model_optimizer_0.zero_grad()\n",
    "        # losses_0[\"loss\"].backward()#debug only\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters_to_train_0, max_norm=1.0)\n",
    "        self.model_optimizer_0.step()\n",
    "\n",
    "        self.set_train()\n",
    "        if self.opt.debug:\n",
    "            self.freeze_params(keys = ['depth_model', 'pose', 'transform_encoder'])#debug only to save_mem\n",
    "        outputs, losses= self.process_batch(inputs)\n",
    "        self.model_optimizer.zero_grad()\n",
    "        # losses[\"loss\"].backward() #debug only\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters_to_train, max_norm=1.0)\n",
    "        self.model_optimizer.step()\n",
    "\n",
    "        duration = time.time() - before_op_time\n",
    "\n",
    "        # phase = batch_idx % self.opt.log_frequency == 0\n",
    "\n",
    "        # # if phase:\n",
    "        # # self.log_time(batch_idx, duration, losses[\"loss\"].cpu().data)\n",
    "        # self.log_time(0, duration, losses[\"loss\"].cpu().data, losses_0[\"loss\"].cpu().data, )\n",
    "        # scalers_to_log = {\n",
    "        #     \"scalar/loss\": losses[\"loss\"],\n",
    "        #     \"scalar/loss_0\": losses_0[\"loss\"],\n",
    "        #     # \"scalar/trans_err\": metric_errs[\"trans_err\"].mean(),\n",
    "        #     # \"scalar/rot_err\": metric_errs[\"rot_err\"].mean()\n",
    "        # }\n",
    "        # for k, v in errs.items():\n",
    "        #     print(f\"scalar/{k}: {v}\")\n",
    "        #     scalers_to_log[f\"scalar/{k}\"] = v\n",
    "        # concat_img, img_order_strs = self.log(\"train\", inputs, outputs, scalers_to_log, compute_vis=True, online_vis=False)\n",
    "        \n",
    "        \n",
    "        # Use the accumulated loss for logging (multiply back to show effective loss)\n",
    "        # effective_loss = losses[\"loss\"] * (self.opt.accumulate_steps / max(1, accumulate_step % self.opt.accumulate_steps))\n",
    "        # self.log_time(batch_idx, duration, effective_loss.cpu().data)\n",
    "        losses_to_log = {\n",
    "            \"loss\": losses[\"loss\"],\n",
    "            \"loss_0\": losses_0[\"loss\"],\n",
    "        }\n",
    "        errs_to_log = {}\n",
    "        errs = self.compute_pose_metrics(inputs, outputs)\n",
    "        for k, v in errs.items():\n",
    "            assert len(v) == len(self.opt.frame_ids)-1, f'{k}: {v}'\n",
    "            # v is a list of torch tensors, average over them\n",
    "            errs_to_log[f\"{k}\"] = torch.mean(torch.stack(v)).item()\n",
    "\n",
    "        self.log_time(self.step, duration, losses[\"loss\"].cpu().data, losses_0[\"loss\"].cpu().data, \n",
    "                        errs_to_log)\n",
    "        scalers_to_log = {**losses_to_log, **errs_to_log}\n",
    "        scalers_to_log = {f\"scalar/{k}\": v for k, v in scalers_to_log.items()}    \n",
    "        # #compute_pose_err\n",
    "        for k, v in errs.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "        # print('trans_err:', outputs[(\"trans_err\", 0)])\n",
    "        # print('rot_err:', outputs[(\"rot_err\", 0)])\n",
    "\n",
    "\n",
    "\n",
    "        # concat_img, img_order_strs = self.log(\"train\", inputs, outputs, \n",
    "        concat_img_list, img_order_strs_list = self.log(\"train\", inputs, outputs, \n",
    "                    scalers_to_log, \n",
    "                    compute_vis=True)\n",
    "\n",
    "        import os, cv2\n",
    "        save_path = \"/mnt/cluster/workspaces/jinjingxu/proj/UniSfMLearner/submodule/Endo_FASt3r/tmp\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        for concat_img, img_order_strs in zip(concat_img_list, img_order_strs_list):\n",
    "        # concat_img, img_order_strs = self.log(\"train\", inputs, outputs, \n",
    "\n",
    "            # remove the first char before the first _\n",
    "            # img_order_strs = img_order_strs.split('_')[1:]\n",
    "            # img_order_strs = '_'.join(img_order_strs)\n",
    "            # full_save_path = os.path.join(save_path, f\"{img_order_strs}.png\")\n",
    "            full_save_path = os.path.join(save_path, f\"{img_order_strs}_{self.step}.png\")\n",
    "            cv2.imwrite(full_save_path, concat_img)\n",
    "            print(f\"saved train_concat_img.png in {full_save_path}\")\n",
    "\n",
    "        self.step += 1\n",
    "        \n",
    "        # break\n",
    "            \n",
    "        self.model_lr_scheduler.step()\n",
    "        self.model_lr_scheduler_0.step()\n",
    "\n",
    "        # if (self.epoch + 1) % self.opt.save_frequency == 0:\n",
    "        #     self.save_model()\n",
    "        # break\n",
    "    \n",
    "\n",
    "\n",
    "    # depth dim:\n",
    "    # 256 320\n",
    "    # 128,160,  \n",
    "    # 64,80,\n",
    "    # 32 40    # ===we go for this\n",
    "\n",
    "\n",
    "\n",
    "# 0.06 0.04 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 97 -16 -43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# self.set_eval()\n",
    "# try:\n",
    "#     # inputs = self.val_iter.next()\n",
    "#     inputs = next(self.val_iter)\n",
    "# except StopIteration:\n",
    "#     self.val_iter = iter(self.val_loader)\n",
    "#     inputs = self.val_iter.next()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # outputs, losses = self.process_batch_val(inputs)\n",
    "#     \"\"\"Pass a minibatch through the network and generate images and losses\n",
    "#     \"\"\"\n",
    "#     for key, ipt in inputs.items():\n",
    "#         inputs[key] = ipt.to(self.device)\n",
    "\n",
    "#     if self.opt.pose_model_type == \"shared\":\n",
    "#         # If we are using a shared encoder for both depth and pose (as advocated\n",
    "#         # in monodepthv1), then all images are fed separately through the depth encoder.\n",
    "#         print('CHECK: self.opt.pose_model_type == \"shared\"')\n",
    "#         all_color_aug = torch.cat([inputs[(\"color_aug\", i, 0)] for i in self.opt.frame_ids])\n",
    "#         all_features = self.models[\"encoder\"](all_color_aug)\n",
    "#         all_features = [torch.split(f, self.opt.batch_size) for f in all_features]\n",
    "\n",
    "#         features = {}\n",
    "#         for i, k in enumerate(self.opt.frame_ids):\n",
    "#             features[k] = [f[i] for f in all_features]\n",
    "\n",
    "#         outputs = self.models[\"depth\"](features[0])\n",
    "#     else:\n",
    "#         print('CHECK: self.opt.pose_model_type != \"shared\"')\n",
    "#         # Otherwise, we only feed the image with frame_id 0 through the depth encoder\n",
    "#         features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
    "#         outputs = self.models[\"depth\"](features)\n",
    "\n",
    "#     if self.opt.predictive_mask:\n",
    "#         outputs[\"predictive_mask\"] = self.models[\"predictive_mask\"](features)\n",
    "\n",
    "#     if self.use_pose_net:\n",
    "#         outputs.update(self.predict_poses(inputs, features, outputs))\n",
    "\n",
    "#     self.generate_images_pred(inputs, outputs)\n",
    "#     losses = self.compute_losses_val(inputs, outputs)\n",
    "\n",
    "#     # return outputs, losses\n",
    "\n",
    "#     self.log(\"val\", inputs, outputs, losses)\n",
    "#     del inputs, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
