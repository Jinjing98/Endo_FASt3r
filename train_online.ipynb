{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place %autoreload 2 at the top of your notebook to ensure it applies to all subsequent imports.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import torch\n",
    "from trainer_end_to_end import Trainer\n",
    "from options import MonodepthOptions\n",
    "import time\n",
    "options = MonodepthOptions()\n",
    "args = [\n",
    "    '--data_path', '/mnt/nct-zfs/TCO-All/SharedDatasets/SCARED_Images_Resized/',\n",
    "    # '--of_samples_num', '3', \n",
    "    '--batch_size', '10', \n",
    "    # '--log_frequency', '2',\n",
    "    # '--log_dir', '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/debug_reloc3r_backbone/relocxr',\n",
    "    '--debug',\n",
    "    ]\n",
    "opts = options.parse_notebook(args)# trainer.set_eval()\n",
    "# opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "trainer = Trainer(opts)\n",
    "print('trainer initialized...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opts.pose_model_type\n",
    "trainer.opt.pose_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(opts)\n",
    "self = trainer #Trainer(opts)\n",
    "\n",
    "\n",
    "# trainer.train()\n",
    "\"\"\"Run the entire training pipeline\n",
    "\"\"\"\n",
    "self.epoch = 0\n",
    "self.step = 0\n",
    "self.start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    # for self.epoch in range(self.opt.num_epochs):\n",
    "    # self.run_epoch(self.epoch)\n",
    "    \"\"\"Run a single epoch of training and validation\n",
    "    \"\"\"\n",
    "    epoch = self.epoch\n",
    "    print(\"Training\")\n",
    "\n",
    "    self.model_optimizer_0.zero_grad()# debug only: save_mem\n",
    "    self.model_optimizer.zero_grad()# debug only: save_mem\n",
    "\n",
    "    # for batch_idx, inputs in enumerate(self.train_loader):\n",
    "\n",
    "    train_iter = iter(self.train_loader)\n",
    "    inputs = next(train_iter)\n",
    "    before_op_time = time.time()\n",
    "\n",
    "    # position\n",
    "    self.set_train_0()\n",
    "    if self.opt.debug:\n",
    "        self.freeze_params(keys = ['position_encoder'])#debug only to save_mem\n",
    "    _, losses_0 = self.process_batch_0(inputs)\n",
    "    self.model_optimizer_0.zero_grad()\n",
    "    # losses_0[\"loss\"].backward()#debug only\n",
    "    torch.nn.utils.clip_grad_norm_(self.parameters_to_train_0, max_norm=1.0)\n",
    "    self.model_optimizer_0.step()\n",
    "\n",
    "    self.set_train()\n",
    "    if self.opt.debug:\n",
    "        self.freeze_params(keys = ['depth_model', 'pose', 'transform_encoder'])#debug only to save_mem\n",
    "    outputs, losses= self.process_batch(inputs)\n",
    "    self.model_optimizer.zero_grad()\n",
    "    # losses[\"loss\"].backward() #debug only\n",
    "    torch.nn.utils.clip_grad_norm_(self.parameters_to_train, max_norm=1.0)\n",
    "    self.model_optimizer.step()\n",
    "\n",
    "    duration = time.time() - before_op_time\n",
    "\n",
    "    # phase = batch_idx % self.opt.log_frequency == 0\n",
    "\n",
    "    # # if phase:\n",
    "    # # self.log_time(batch_idx, duration, losses[\"loss\"].cpu().data)\n",
    "    # self.log_time(0, duration, losses[\"loss\"].cpu().data, losses_0[\"loss\"].cpu().data, )\n",
    "    # scalers_to_log = {\n",
    "    #     \"scalar/loss\": losses[\"loss\"],\n",
    "    #     \"scalar/loss_0\": losses_0[\"loss\"],\n",
    "    #     # \"scalar/trans_err\": metric_errs[\"trans_err\"].mean(),\n",
    "    #     # \"scalar/rot_err\": metric_errs[\"rot_err\"].mean()\n",
    "    # }\n",
    "    # for k, v in errs.items():\n",
    "    #     print(f\"scalar/{k}: {v}\")\n",
    "    #     scalers_to_log[f\"scalar/{k}\"] = v\n",
    "    # concat_img, img_order_strs = self.log(\"train\", inputs, outputs, scalers_to_log, compute_vis=True, online_vis=False)\n",
    "    \n",
    "    \n",
    "    # Use the accumulated loss for logging (multiply back to show effective loss)\n",
    "    # effective_loss = losses[\"loss\"] * (self.opt.accumulate_steps / max(1, accumulate_step % self.opt.accumulate_steps))\n",
    "    # self.log_time(batch_idx, duration, effective_loss.cpu().data)\n",
    "    losses_to_log = {\n",
    "        \"loss\": losses[\"loss\"],\n",
    "        \"loss_0\": losses_0[\"loss\"],\n",
    "    }\n",
    "    errs_to_log = {}\n",
    "    errs = self.compute_pose_metrics(inputs, outputs)\n",
    "    for k, v in errs.items():\n",
    "        assert len(v) == len(self.opt.frame_ids)-1, f'{k}: {v}'\n",
    "        # v is a list of torch tensors, average over them\n",
    "        errs_to_log[f\"{k}\"] = torch.mean(torch.stack(v)).item()\n",
    "\n",
    "    self.log_time(self.step, duration, losses[\"loss\"].cpu().data, losses_0[\"loss\"].cpu().data, \n",
    "                    errs_to_log)\n",
    "    scalers_to_log = {**losses_to_log, **errs_to_log}\n",
    "    scalers_to_log = {f\"scalar/{k}\": v for k, v in scalers_to_log.items()}\n",
    "\n",
    "    concat_img, img_order_strs = self.log(\"train\", inputs, outputs, \n",
    "                scalers_to_log, \n",
    "                compute_vis=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # #compute_pose_err\n",
    "    for k, v in errs.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # print('trans_err:', outputs[(\"trans_err\", 0)])\n",
    "    # print('rot_err:', outputs[(\"rot_err\", 0)])\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    import os, cv2\n",
    "    save_path = \"/mnt/cluster/workspaces/jinjingxu/proj/UniSfMLearner/submodule/Endo_FASt3r/\"\n",
    "    # remove the first char before the first _\n",
    "    # img_order_strs = img_order_strs.split('_')[1:]\n",
    "    # img_order_strs = '_'.join(img_order_strs)\n",
    "    save_path = os.path.join(save_path, f\"{img_order_strs}.png\")\n",
    "    cv2.imwrite(save_path, concat_img)\n",
    "    print(f\"saved train_concat_img.png in {save_path}\")\n",
    "\n",
    "    self.step += 1\n",
    "    \n",
    "    # break\n",
    "        \n",
    "    self.model_lr_scheduler.step()\n",
    "    self.model_lr_scheduler_0.step()\n",
    "\n",
    "    # if (self.epoch + 1) % self.opt.save_frequency == 0:\n",
    "    #     self.save_model()\n",
    "    # break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# self.set_eval()\n",
    "# try:\n",
    "#     # inputs = self.val_iter.next()\n",
    "#     inputs = next(self.val_iter)\n",
    "# except StopIteration:\n",
    "#     self.val_iter = iter(self.val_loader)\n",
    "#     inputs = self.val_iter.next()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # outputs, losses = self.process_batch_val(inputs)\n",
    "#     \"\"\"Pass a minibatch through the network and generate images and losses\n",
    "#     \"\"\"\n",
    "#     for key, ipt in inputs.items():\n",
    "#         inputs[key] = ipt.to(self.device)\n",
    "\n",
    "#     if self.opt.pose_model_type == \"shared\":\n",
    "#         # If we are using a shared encoder for both depth and pose (as advocated\n",
    "#         # in monodepthv1), then all images are fed separately through the depth encoder.\n",
    "#         print('CHECK: self.opt.pose_model_type == \"shared\"')\n",
    "#         all_color_aug = torch.cat([inputs[(\"color_aug\", i, 0)] for i in self.opt.frame_ids])\n",
    "#         all_features = self.models[\"encoder\"](all_color_aug)\n",
    "#         all_features = [torch.split(f, self.opt.batch_size) for f in all_features]\n",
    "\n",
    "#         features = {}\n",
    "#         for i, k in enumerate(self.opt.frame_ids):\n",
    "#             features[k] = [f[i] for f in all_features]\n",
    "\n",
    "#         outputs = self.models[\"depth\"](features[0])\n",
    "#     else:\n",
    "#         print('CHECK: self.opt.pose_model_type != \"shared\"')\n",
    "#         # Otherwise, we only feed the image with frame_id 0 through the depth encoder\n",
    "#         features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
    "#         outputs = self.models[\"depth\"](features)\n",
    "\n",
    "#     if self.opt.predictive_mask:\n",
    "#         outputs[\"predictive_mask\"] = self.models[\"predictive_mask\"](features)\n",
    "\n",
    "#     if self.use_pose_net:\n",
    "#         outputs.update(self.predict_poses(inputs, features, outputs))\n",
    "\n",
    "#     self.generate_images_pred(inputs, outputs)\n",
    "#     losses = self.compute_losses_val(inputs, outputs)\n",
    "\n",
    "#     # return outputs, losses\n",
    "\n",
    "#     self.log(\"val\", inputs, outputs, losses)\n",
    "#     del inputs, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
